{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import tqdm\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import scipy.misc\n",
    "import scipy.io as sio\n",
    "import fcn\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "max_iteration=100000\n",
    "lr=1.0e-14\n",
    "momentum=0.99\n",
    "weight_decay=0.0005\n",
    "interval_validate=4000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# to reproduce same results\n",
    "torch.manual_seed(1337)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        if(train):\n",
    "            rfile = '200'\n",
    "        else :\n",
    "            rfile = '201'\n",
    "        ldir = root_dir + 'VOCdevkit/VOC2010/JPEGImages/'\n",
    "        sdir = root_dir + 'pascal_data/pascal_data/SegmentationPart/'\n",
    "#         pdir = root_dir + 'pascal_data/pascal_data/PersonJoints/'\n",
    "#         self.transform = transform\n",
    "        self.img = []\n",
    "        self.seg = []\n",
    "#         self.mat = []\n",
    "        \n",
    "        for line in glob.glob(sdir+rfile+'*.png'):\n",
    "            line = line.split(\"/\")[-1].split(\".\")[-2]\n",
    "#                     print(line)\n",
    "            self.img.append(ldir+line+'.jpg')\n",
    "            self.seg.append(sdir+line+'.png')\n",
    "#                     self.mat.append(pdir+line+'.mat')\n",
    "#             PUT DATA IN CORRESPONDING VARS\n",
    "            \n",
    "            \n",
    "#             self.label.append(ord(file_path.split('/')[-2]) - ord('A')) #ord makes A,B,C.. to 0,1,2,.. respectively\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        return len(self.img)\n",
    "       \n",
    "    mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n",
    "    def transform(self, img, lbl):\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean_bgr\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        return img, lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "#         print(idx)\n",
    "        image = PIL.Image.open(self.img[idx])\n",
    "        image = np.array(image,dtype=np.uint8)\n",
    "        segment = PIL.Image.open(self.seg[idx])\n",
    "        segment = np.array(segment,dtype=np.uint8)\n",
    "#         print(image.shape,segment.shape)\n",
    "#         poset,paft = genmat(self.mat[idx],segment)\n",
    "        if self.transform is None:\n",
    "            return (image,segment)\n",
    "        else:\n",
    "            img_transformed,segment = self.transform(image,segment)\n",
    "#             RETURN VARS\n",
    "            return (img_transformed,segment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "class Seg_test(VOCClassSegBase):\n",
    "\n",
    "    # XXX: It must be renamed to benchmark.tar to be extracted.\n",
    "    url = 'http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz'  # NOQA\n",
    "\n",
    "    def __init__(self, root, split='train', transform=False):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self._transform = transform\n",
    "\n",
    "        self.files = collections.defaultdict(list)\n",
    "        for split in ['train', 'val']:\n",
    "            imgsets_file = osp.join(self.root, 'pascal_data/pascal_data/%s_seg.txt' % split)\n",
    "            img_id_list = [did.strip() for did in open(imgsets_file)]\n",
    "            np.random.shuffle(img_id_list)\n",
    "            img_id_list = img_id_list[:0.1*len(img_id_list)]\n",
    "            for did in open(img_id_list):\n",
    "                img_file = osp.join(self.root, 'VOCdevkit/VOC2010/JPEGImages/%s.jpg' % did)\n",
    "                lbl_file = osp.join(self.root, 'pascal_data/pascal_data/SegmentationPart/%s.png' % did)\n",
    "                self.files[split].append({\n",
    "                    'img': img_file,\n",
    "                    'lbl': lbl_file,\n",
    "                })\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_file = self.files[self.split][index]\n",
    "        # load image\n",
    "        img_file = data_file['img']\n",
    "        img = PIL.Image.open(img_file)\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        # load label\n",
    "        lbl_file = data_file['lbl']\n",
    "        mat = PIL.Image.open(lbl_file)\n",
    "        lbl =  np.array(mat, dtype=np.uint8)\n",
    "        if self._transform:\n",
    "            return self.transform(img, lbl)\n",
    "        else:\n",
    "            return img, lbl\n",
    "        \n",
    "class PAFloader(VOCClassSegBase):\n",
    "\n",
    "    # XXX: It must be renamed to benchmark.tar to be extracted.\n",
    "    url = 'http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz'  # NOQA\n",
    "\n",
    "    def __init__(self, root, split='train', transform=False):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self._transform = transform\n",
    "\n",
    "        self.files = collections.defaultdict(list)\n",
    "        for split in ['train', 'val']:\n",
    "            imgsets_file = osp.join(self.root, 'pascal_data/pascal_data/%s_idnew.txt' % split)\n",
    "            for did in open(imgsets_file):\n",
    "                did = did.strip()\n",
    "                img_file = osp.join(self.root, 'VOCdevkit/VOC2010/JPEGImages/%s.jpg' % did)\n",
    "                lbl_file = osp.join(self.root, 'pascal_data/pascal_data/PersonJoints/%s.mat' % did)\n",
    "                self.files[split].append({\n",
    "                    'img': img_file,\n",
    "                    'lbl': lbl_file,\n",
    "                })\n",
    "    \n",
    "    def genmat(path,image):\n",
    "        H,W,_ = image.shape\n",
    "        Hn = np.ceil(H/32).astype(np.int64)\n",
    "        Wn = np.ceil(W/32).astype(np.int64)\n",
    "        mat = sio.loadmat(path)\n",
    "        limbs = [[0,1],[1,2],[2,3],[3,4],[1,5],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13]]\n",
    "        out = np.zeros((14,Hn,Wn))\n",
    "        paf = np.zeros((26,Hn,Wn))\n",
    "        x, y = np.meshgrid(np.arange(Wn), np.arange(Hn))\n",
    "        for human in mat['joints'][0]:\n",
    "            poselist = np.around(human[:,:-1]).astype(np.int64)\n",
    "            poselist[:,0] = poselist[:,0]*Hn/H\n",
    "            poselist[:,1] = poselist[:,1]*Wn/W\n",
    "            vis = human[:,2]\n",
    "            #PAF GT\n",
    "            for (i,limb) in enumerate(limbs):\n",
    "                p1 = poselist[limb[0],:]\n",
    "                p2 = poselist[limb[1],:]\n",
    "                dvec = (p2-p1)/np.linalg.norm(p2-p1)\n",
    "                if not (vis[limb[0]]==0 or vis[limb[1]]==0):\n",
    "    #             if (np.all(p1>0) and np.all(p2>0)):\n",
    "                    #APPROX RECON\n",
    "                    vecx = x - p1[0]\n",
    "                    vecy = y - p1[1]\n",
    "                    dot = vecx*dvec[0] + vecy*dvec[1]\n",
    "                    perp2 = vecx**2+vecy**2-dot**2\n",
    "                    boolmat = (dot>0) & (dot<np.linalg.norm(p2-p1)) & (perp2<np.linalg.norm(p2-p1)*0.3) #sigma^2\n",
    "                    paf[2*i][boolmat] = dvec[0]\n",
    "                    paf[2*i+1][boolmat] = dvec[1]\n",
    "            #POSE GT\n",
    "            for (i,pose) in enumerate(poselist):\n",
    "                tmp = np.exp(-((x-pose[0])**2 + (y-pose[1])**2)/(2.0*1.0))\n",
    "                out[i] = np.maximum(out[i],tmp)\n",
    "        return out,paf\n",
    "\n",
    "    \n",
    "    def transform(self, img, pose, paf):\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean_bgr\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        paf = torch.from_numpy(paf).float()\n",
    "        return img, pose, paf\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_file = self.files[self.split][index]\n",
    "        # load image\n",
    "        img_file = data_file['img']\n",
    "        img = PIL.Image.open(img_file)\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        # load label\n",
    "        lbl_file = data_file['lbl']\n",
    "        pose, paf = genmat(lbl_file,img)\n",
    "        if self._transform:\n",
    "            return self.transform(img, pose, paf)\n",
    "        else:\n",
    "            return img, pose, paf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 4871\n",
      "Size of test dataset: 1889\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((224,224)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='/extra_data/ayushya/', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='/extra_data/ayushya/', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "print('Size of train dataset: %d' % len(train_dataset))\n",
    "print('Size of test dataset: %d' % len(test_dataset))\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# Create loaders for the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class suhpos(nn.Module):\n",
    "    def __init__(self, n_class=7):\n",
    "        super(suhpos, self).__init__()\n",
    "        # conv1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n",
    "\n",
    "        # conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n",
    "\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n",
    "\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n",
    "\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n",
    "\n",
    "        # fc6\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc7\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        self.score_fr = nn.Conv2d(4096, n_class, 1)\n",
    "        self.upscore = nn.ConvTranspose2d(n_class, n_class, 64, stride=32,\n",
    "                                          bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.relu1_1(self.conv1_1(h))\n",
    "        h = self.relu1_2(self.conv1_2(h))\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.relu2_1(self.conv2_1(h))\n",
    "        h = self.relu2_2(self.conv2_2(h))\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.relu3_1(self.conv3_1(h))\n",
    "        h = self.relu3_2(self.conv3_2(h))\n",
    "        h = self.relu3_3(self.conv3_3(h))\n",
    "        h = self.pool3(h)\n",
    "\n",
    "        h = self.relu4_1(self.conv4_1(h))\n",
    "        h = self.relu4_2(self.conv4_2(h))\n",
    "        h = self.relu4_3(self.conv4_3(h))\n",
    "        h = self.pool4(h)\n",
    "\n",
    "        h = self.relu5_1(self.conv5_1(h))\n",
    "        h = self.relu5_2(self.conv5_2(h))\n",
    "        h = self.relu5_3(self.conv5_3(h))\n",
    "        h = self.pool5(h)\n",
    "\n",
    "        h = self.relu6(self.fc6(h))\n",
    "        h = self.drop6(h)\n",
    "\n",
    "        h = self.relu7(self.fc7(h))\n",
    "        h = self.drop7(h)\n",
    "\n",
    "        h = self.score_fr(h)\n",
    "\n",
    "        h = self.upscore(h)\n",
    "        h = h[:, :, 19:19 + x.size()[2], 19:19 + x.size()[3]].contiguous()\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
    "            l2.bias.data = l1.bias.data.view(l2.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def VGG16(pretrained=False):\n",
    "    model = torchvision.models.vgg16(pretrained=False)\n",
    "    if not pretrained:\n",
    "        return model\n",
    "    model_file = _get_vgg16_pretrained_model()\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def _get_vgg16_pretrained_model():\n",
    "    return fcn.data.cached_download(\n",
    "#         https://drive.google.com/uc?export=download&confirm=pNaP&id=0B9P1L--7Wd2vLTJZMXpIRkVVRFk\n",
    "        url='http://drive.google.com/uc?id=0B9P1L--7Wd2vLTJZMXpIRkVVRFk',\n",
    "        path=osp.expanduser('~/data/models/pytorch/vgg16_from_caffe.pth'),\n",
    "        md5='aa75b158f4181e7f6230029eb96c1b13',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permission denied: http://drive.google.com/uc?id=0B9P1L--7Wd2vLTJZMXpIRkVVRFk\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "nothing to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a7c02309aee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model = torchfcn.models.FCN8s(n_class=21)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# vgg16 = models.vgg16(pretrained=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuhpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_params_from_vgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c3d6cea6d45b>\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(pretrained)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_vgg16_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/abhiagwl/miniconda2/envs/abhinav/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/abhiagwl/miniconda2/envs/abhinav/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# try the legacy loader first, which only works if f is a tarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/abhiagwl/miniconda2/envs/abhinav/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAX_FORMAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/abhiagwl/miniconda2/envs/abhinav/lib/python2.7/tarfile.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nothing to open\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: nothing to open"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "# DEFINE MODEL\n",
    "# model = torchfcn.models.FCN8s(n_class=21)\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16 = VGG16(pretrained=True)\n",
    "model = suhpos()\n",
    "model.copy_params_from_vgg16(vgg16)\n",
    "# print(model)\n",
    "\n",
    "resume = 0\n",
    "\n",
    "start_epoch = 0\n",
    "start_iteration = 0\n",
    "if cuda:\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOSS\n",
    "loss1 = nn.MSELoss()\n",
    "loss2 = nn.MSELoss()\n",
    "\n",
    "def cross_entropy2d(input, target, weight=None, size_average=False):\n",
    "    # input: (n, c, h, w), target: (n, h, w)\n",
    "    n, c, h, w = input.size()\n",
    "    # log_p: (n, c, h, w)\n",
    "    log_p = F.log_softmax(input)\n",
    "    # log_p: (n*h*w, c)\n",
    "    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n",
    "    log_p = log_p.view(-1, c)\n",
    "    # target: (n*h*w,)\n",
    "    mask = target >= 0\n",
    "    target = target[mask]\n",
    "    loss = F.nll_loss(log_p, target, weight=weight, size_average=False)\n",
    "    if size_average:\n",
    "        loss /= mask.data.sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameters(model, bias=False):\n",
    "    import torch.nn as nn\n",
    "    modules_skipped = (\n",
    "        nn.ReLU,\n",
    "        nn.MaxPool2d,\n",
    "        nn.Dropout2d,\n",
    "        nn.Sequential,\n",
    "        suhpos,\n",
    "    )\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if bias:\n",
    "                yield m.bias\n",
    "            else:\n",
    "                yield m.weight\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            # weight is frozen because it is just a bilinear upsampling\n",
    "            if bias:\n",
    "                assert m.bias is None\n",
    "        elif isinstance(m, modules_skipped):\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError('Unexpected module: %s' % str(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optim = torch.optim.SGD(\n",
    "    [\n",
    "            {'params': get_parameters(model, bias=False)},\n",
    "            {'params': get_parameters(model, bias=True),\n",
    "             'lr': lr * 2, 'weight_decay': 0},\n",
    "    ],\n",
    "    lr=lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay)\n",
    "# print(\"optimizer made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_class = 7\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def label_accuracy_score(label_trues, label_preds, n_class):\n",
    "    \"\"\"Returns accuracy score evaluation result.\n",
    "      - overall accuracy\n",
    "      - mean accuracy\n",
    "      - mean IU\n",
    "      - fwavacc\n",
    "    \"\"\"\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ones  = np.ones((1,7,50,50))\n",
    "for i in xrange(7):\n",
    "    ones[0,i,:,:]+=i**2\n",
    "ones = torch.from_numpy(ones).long()\n",
    "out = Variable(ones).data.max(1)[1].cpu().numpy()[:,:,:]\n",
    "\n",
    "onez_ = np.ones((1,50,50)).astype(np.int64) * 6\n",
    "\n",
    "label_accuracy_score(out,onez_, 7)\n",
    "\n",
    "onez_ = np.zeros((1,50,50)).astype(np.int64)\n",
    "\n",
    "label_accuracy_score(out,onez_,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "def train_model():\n",
    "    lval = 1000.0\n",
    "    max_epoch = int(math.ceil(1. * max_iteration / len(train_loader)))\n",
    "#     print(max_epoch)\n",
    "    for epoch in tqdm.trange(0, max_epoch, desc='Train', ncols=80):\n",
    "#         INSERT TARGETS\n",
    "        for batch_idx, (data, target) in tqdm.tqdm( enumerate(train_loader), \n",
    "                                                   total=len(train_loader),\n",
    "                                                   desc=\"Train epoch: \"+str(epoch) ,\n",
    "                                                   ncols=80,\n",
    "                                                   leave=False):\n",
    "#         for batch_idx, (data, target) in enumerate(train_loader):  \n",
    "            iteration = batch_idx + epoch * len(train_loader)\n",
    "            if iteration >= max_iteration:\n",
    "                break\n",
    "\n",
    "#             print(iteration)\n",
    "#             VALIDATE\n",
    "#             if iteration % interval_validate == 0:\n",
    "#                 validate(iteration)\n",
    "\n",
    "#             MODIFY FOR TARGETS\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optim.zero_grad()\n",
    "#             MODIFY FOR OUTPUTS\n",
    "            seg = model(data)\n",
    "\n",
    "            loss = cross_entropy2d(seg, target)\n",
    "            loss /= len(data)\n",
    "            if np.isnan(float(loss.data[0])):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "#             METRICS\n",
    "#           metrics = []\n",
    "#                     lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "#                     lbl_true = target.data.cpu().numpy()\n",
    "#                     for lt, lp in zip(lbl_true, lbl_pred):\n",
    "#                         acc, acc_cls, mean_iu, fwavacc = \\\n",
    "#                             torchfcn.utils.label_accuracy_score(\n",
    "#                                 [lt], [lp], n_class=n_class)\n",
    "#                         metrics.append((acc, acc_cls, mean_iu, fwavacc))\n",
    "#                     metrics = np.mean(metrics, axis=0)\n",
    "\n",
    "            metrics = []\n",
    "            lbl_pred = seg.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "            lbl_true = target.data.cpu().numpy()\n",
    "            for lt, lp in zip(lbl_true, lbl_pred):\n",
    "                acc, acc_cls, mean_iu, fwavacc = \\\n",
    "                    label_accuracy_score(\n",
    "                        [lt], [lp], n_class=n_class)\n",
    "                metrics.append((acc, acc_cls, mean_iu, fwavacc))\n",
    "            metrics = np.mean(metrics, axis=0)\n",
    "            \n",
    "            if iteration%100 == 0:\n",
    "                print(\"loss :\", loss.data[0], \"Metrics : \", metrics.tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|                                             | 0/21 [00:00<?, ?it/s]\n",
      "Train epoch: 0:   0%|                                  | 0/4871 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 62, in _pin_memory_loop\n    batch = pin_memory_batch(batch)\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 123, in pin_memory_batch\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 117, in pin_memory_batch\n    return batch.pin_memory()\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/tensor.py\", line 82, in pin_memory\n    return type(self)().set_(storage.pin_memory()).view_as(self)\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/storage.py\", line 84, in pin_memory\n    return type(self)(self.size(), allocator=allocator).copy_(self)\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/THCCachingHostAllocator.cpp:258\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-20edab97d712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-fcf43dfd031d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                    \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train epoch: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                    \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                    leave=False):\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#         for batch_idx, (data, target) in enumerate(train_loader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/tqdm/_tqdm.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 62, in _pin_memory_loop\n    batch = pin_memory_batch(batch)\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 123, in pin_memory_batch\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 117, in pin_memory_batch\n    return batch.pin_memory()\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/tensor.py\", line 82, in pin_memory\n    return type(self)().set_(storage.pin_memory()).view_as(self)\n  File \"/users/gpu/ayushya/miniconda2/envs/ayushya/lib/python2.7/site-packages/torch/storage.py\", line 84, in pin_memory\n    return type(self)(self.size(), allocator=allocator).copy_(self)\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/THCCachingHostAllocator.cpp:258\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
