{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import scipy.misc\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "max_iteration=100000\n",
    "lr=1.0e-14\n",
    "momentum=0.99\n",
    "weight_decay=0.0005\n",
    "interval_validate=4000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# to reproduce same results\n",
    "torch.manual_seed(1337)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genmatdemo(path,segment):\n",
    "    W,H = segment.size\n",
    "    segment = np.array(segment.getdata()).reshape(H,W)\n",
    "#     print(segment.shape)\n",
    "    mat = sio.loadmat(path)\n",
    "    #ADD A POSE FOR HIP CENTER? OR HEURISTIC ON SEGMENT BOUNDS\n",
    "    limbs = [[0,1],[1,2],[2,3],[3,4],[1,5],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13]]\n",
    "    out = np.zeros((14,H,W))\n",
    "#     print(out[0].shape)\n",
    "    pafx = np.zeros((H,W))\n",
    "#     print(pafx.shape)\n",
    "    pafy = np.zeros((H,W))\n",
    "    x, y = np.meshgrid(np.arange(W), np.arange(H))\n",
    "#     y = np.flipud(y)\n",
    "#     print(x.shape)\n",
    "    for human in mat['joints'][0]:\n",
    "        poselist = np.around(human[:,:-1]).astype(np.int64)\n",
    "        vis = human[:,2]\n",
    "        #PAF GT\n",
    "        for limb in limbs:\n",
    "            p1 = poselist[limb[0],:]\n",
    "            p2 = poselist[limb[1],:]\n",
    "            dvec = (p2-p1)/np.linalg.norm(p2-p1)\n",
    "            if not (vis[limb[0]]==0 or vis[limb[1]]==0):\n",
    "#             if (np.all(p1>0) and np.all(p2>0)):\n",
    "                #APPROX RECON\n",
    "                vecx = x - p1[0]\n",
    "                vecy = y - p1[1]\n",
    "                dot = vecx*dvec[0] + vecy*dvec[1]\n",
    "                perp2 = vecx**2+vecy**2-dot**2\n",
    "                boolmat = (dot>0) & (dot<np.linalg.norm(p2-p1)) & (perp2<np.linalg.norm(p2-p1)*0.3) #sigma^2\n",
    "                pafx[boolmat] = 255*dvec[0]\n",
    "                pafy[boolmat] = 255*dvec[1]\n",
    "#             else:\n",
    "#                 mp = np.around((p1+p2)/2.0).astype(np.uint8)\n",
    "#                 midval = segment[mp[1],mp[0]]\n",
    "#                 pafx[segment==midval] = dvec[0]\n",
    "#                 pafy[segment==midval] = dvec[1]\n",
    "        #POSE GT\n",
    "        for (i,pose) in enumerate(poselist):\n",
    "            tmp = 255*np.exp(-((x-pose[0])**2 + (y-pose[1])**2)/(2.0*50.0))\n",
    "            out[0] = np.maximum(out[0],tmp)\n",
    "#             print(human[i])\n",
    "        out[0]=np.maximum(out[0],pafx+pafy)\n",
    "    return out,pafx,pafy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/gpu/abhiagwl/miniconda2/envs/abhinav/lib/python2.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4e4d47c550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM8AAAD8CAYAAADQb/BcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmPx/X1ffZwOT/owAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e51afdb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM8AAAD8CAYAAADQb/BcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHB6zTA6PWSaHxyzT/wNdABclHEYFZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e51b06c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEMO\n",
    "segment = PIL.Image.open('/extra_data/ayushya/pascal_data/pascal_data/SegmentationPart/2009_000544.png')\n",
    "out,pafx,pafy = genmatdemo('/extra_data/ayushya/pascal_data/pascal_data/PersonJoints/2009_000544.mat',segment)\n",
    "plt.figure()\n",
    "plt.imshow(out[0])\n",
    "# plt.figure()\n",
    "# plt.imshow(pafx)\n",
    "plt.figure()\n",
    "plt.imshow(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genmat(path,segment):\n",
    "    W,H = segment.size\n",
    "    segment = np.array(segment.getdata()).reshape(H,W)\n",
    "#     print(segment.shape)\n",
    "    mat = sio.loadmat(path)\n",
    "    #ADD A POSE FOR HIP CENTER? OR HEURISTIC ON SEGMENT BOUNDS\n",
    "    limbs = [[0,1],[1,2],[2,3],[3,4],[1,5],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13]]\n",
    "    out = np.zeros((14,H,W))\n",
    "#     print(out[0].shape)\n",
    "    paf = np.zeros((13,2,H,W))\n",
    "#     print(pafx.shape)\n",
    "    x, y = np.meshgrid(np.arange(W), np.arange(H))\n",
    "#     y = np.flipud(y)\n",
    "#     print(x.shape)\n",
    "    for human in mat['joints'][0]:\n",
    "        poselist = np.around(human[:,:-1]).astype(np.int64)\n",
    "        vis = human[:,2]\n",
    "        #PAF GT\n",
    "        for (i,limb) in enumerate(limbs):\n",
    "            p1 = poselist[limb[0],:]\n",
    "            p2 = poselist[limb[1],:]\n",
    "            dvec = (p2-p1)/np.linalg.norm(p2-p1)\n",
    "            if not (vis[limb[0]]==0 or vis[limb[1]]==0):\n",
    "#             if (np.all(p1>0) and np.all(p2>0)):\n",
    "                #APPROX RECON\n",
    "                vecx = x - p1[0]\n",
    "                vecy = y - p1[1]\n",
    "                dot = vecx*dvec[0] + vecy*dvec[1]\n",
    "                perp2 = vecx**2+vecy**2-dot**2\n",
    "                boolmat = (dot>0) & (dot<np.linalg.norm(p2-p1)) & (perp2<np.linalg.norm(p2-p1)*0.3) #sigma^2\n",
    "                paf[i][0][boolmat] = 255*dvec[0]\n",
    "                paf[i][1][boolmat] = 255*dvec[1]\n",
    "#             else:\n",
    "#                 mp = np.around((p1+p2)/2.0).astype(np.uint8)\n",
    "#                 midval = segment[mp[1],mp[0]]\n",
    "#                 pafx[segment==midval] = dvec[0]\n",
    "#                 pafy[segment==midval] = dvec[1]\n",
    "        #POSE GT\n",
    "        for (i,pose) in enumerate(poselist):\n",
    "            tmp = 255*np.exp(-((x-pose[0])**2 + (y-pose[1])**2)/(2.0*50.0))\n",
    "            out[i] = np.maximum(out[i],tmp)\n",
    "#             print(human[i])\n",
    "#         out[0]=np.maximum(out[0],pafx+pafy)\n",
    "    return out,paf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 2, 500, 375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/gpu/abhiagwl/miniconda2/envs/abhinav/lib/python2.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# DEMO\n",
    "segment = PIL.Image.open('/extra_data/ayushya/pascal_data/pascal_data/SegmentationPart/2009_000544.png')\n",
    "out,paf = genmat('/extra_data/ayushya/pascal_data/pascal_data/PersonJoints/2009_000544.mat',segment)\n",
    "print(paf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ma\n",
    "U = pafx[0]\n",
    "V = pafy[0]\n",
    "segment = PIL.Image.open('/extra_data/ayushya/pascal_data/pascal_data/SegmentationPart/2009_000544.png')\n",
    "W,H = segment.size\n",
    "segment = np.array(segment.getdata()).reshape(H,W)\n",
    "X, Y = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))\n",
    "# 1\n",
    "plt.figure()\n",
    "plt.imshow(segment, alpha = .5)\n",
    "s = 5\n",
    "Q = plt.quiver(X[::s], Y[::s], U[::s], V[::s], \n",
    "               scale=100, headaxislength=4, alpha=.5, width=0.001, color='r')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        if(train):\n",
    "            rfile = root_dir+'pascal_data/pascal_data/train_idnew.txt'\n",
    "        else :\n",
    "            rfile = root_dir+'pascal_data/pascal_data/val_idnew.txt'\n",
    "        ldir = root_dir + 'VOCdevkit/VOC2010/JPEGImages/'\n",
    "        sdir = root_dir + 'pascal_data/pascal_data/SegmentationPart/'\n",
    "        pdir = root_dir + 'pascal_data/pascal_data/PersonJoints/'\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        self.seg = []\n",
    "        self.mat = []\n",
    "        \n",
    "        with open(rfile,'r') as f:\n",
    "            for line in f:\n",
    "                    line = line[:-1]\n",
    "#                     print(line)\n",
    "                    self.img.append(ldir+line+'.jpg')\n",
    "                    self.seg.append(sdir+line+'.png')\n",
    "                    self.mat.append(pdir+line+'.mat')\n",
    "#             PUT DATA IN CORRESPONDING VARS\n",
    "            \n",
    "            \n",
    "#             self.label.append(ord(file_path.split('/')[-2]) - ord('A')) #ord makes A,B,C.. to 0,1,2,.. respectively\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        image = PIL.Image.open(self.img[idx])\n",
    "        segment = PIL.Image.open(self.seg[idx])\n",
    "        poset,paft = genmat(self.mat[idx],segment)\n",
    "        if self.transform is None:\n",
    "            return (image,segment,poset,paft)\n",
    "        else:\n",
    "            img_transformed = self.transform(image)\n",
    "#             RETURN VARS\n",
    "            return (img_transformed,segment,poset,paft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 1696\n",
      "Size of test dataset: 1797\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((224,224)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='/extra_data/ayushya/', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='/extra_data/ayushya/', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "print('Size of train dataset: %d' % len(train_dataset))\n",
    "print('Size of test dataset: %d' % len(test_dataset))\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# Create loaders for the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class suhpos(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(suhpos, self).__init__()\n",
    "        self.features = pretrained_model.features\n",
    "        self.poselayer = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 14, kernel_size=(1, 1), stride=(1, 1)),\n",
    "        )\n",
    "        self.paflayer = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 13, kernel_size=(1, 1), stride=(1, 1)),\n",
    "        )\n",
    "        self.seglayer = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 7, kernel_size=(1, 1), stride=(1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        pose = self.poselayer(f)\n",
    "        paf = self.paflayer(f)\n",
    "        seg = self.seglayer(f)\n",
    "        return pose,paf,seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# DEFINE MODEL\n",
    "# model = torchfcn.models.FCN8s(n_class=21)\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "model = suhpos(vgg19)\n",
    "print(model)\n",
    "\n",
    "resume = 0\n",
    "\n",
    "start_epoch = 0\n",
    "start_iteration = 0\n",
    "if cuda:\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOSS\n",
    "loss1 = nn.MSELoss()\n",
    "loss2 = nn.MSELoss()\n",
    "\n",
    "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
    "    # input: (n, c, h, w), target: (n, h, w)\n",
    "    n, c, h, w = input.size()\n",
    "    # log_p: (n, c, h, w)\n",
    "    log_p = F.log_softmax(input)\n",
    "    # log_p: (n*h*w, c)\n",
    "#     log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "#     log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n",
    "#     log_p = log_p.view(-1, c)\n",
    "    # target: (n*h*w,)\n",
    "#     mask = target >= 0\n",
    "#     target = target[mask]\n",
    "    loss = F.nll_loss(log_p, target, weight=weight, size_average=False)\n",
    "    if size_average:\n",
    "        loss /= mask.data.sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optim = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "def validate(iteration):\n",
    "        val_loss = 0\n",
    "        label_trues, label_preds = [], []\n",
    "        for batch_idx, (data, target) in tqdm.tqdm(\n",
    "                enumerate(test_loader), total=len(test_loader),\n",
    "                desc='Valid iteration=%d' % iteration, ncols=80,\n",
    "                leave=False):\n",
    "            \n",
    "#             INSERT TARGETS\n",
    "            if self.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            score = self.model(data)\n",
    "\n",
    "            loss = cross_entropy2d(score, target)\n",
    "            if np.isnan(float(loss.data[0])):\n",
    "                raise ValueError('loss is nan while validating')\n",
    "            \n",
    "            val_loss += float(loss.data[0]) / len(data)\n",
    "\n",
    "#             imgs = data.data.cpu()\n",
    "#             lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "#             lbl_true = target.data.cpu()\n",
    "                \n",
    "#         SAVE IMAGES        \n",
    "#         out = \"val_out/\"\n",
    "#         if not osp.exists(out):\n",
    "#             os.makedirs(out)\n",
    "#         out_file = osp.join(out, 'iter%.jpg' % iteration)\n",
    "#         scipy.misc.imsave(out_file, image)\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "def train_model():\n",
    "    max_epoch = int(math.ceil(1. * max_iter / len(train_loader)))\n",
    "    for epoch in tqdm.trange(0, max_epoch,\n",
    "                             desc='Train', ncols=80):\n",
    "#         INSERT TARGETS\n",
    "        for batch_idx, (data, target) in tqdm.tqdm( \n",
    "                enumerate(train_loader), total=len(train_loader),\n",
    "                desc='Train epoch=%d' % epoch, ncols=80, leave=False):\n",
    "            iteration = batch_idx + epoch * len(train_loader)\n",
    "            \n",
    "#             VALIDATE\n",
    "#             if iteration % interval_validate == 0:\n",
    "#                 validate(iteration)\n",
    "\n",
    "#             MODIFY FOR TARGETS\n",
    "            if self.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optim.zero_grad()\n",
    "#             MODIFY FOR OUTPUTS\n",
    "            pose,paf,seg = model(data)\n",
    "\n",
    "            loss = cross_entropy2d(seg, )\n",
    "            loss /= len(data)\n",
    "            if np.isnan(float(loss.data[0])):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "#             METRICS\n",
    "#             metrics = []\n",
    "#             lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "#             lbl_true = target.data.cpu().numpy()\n",
    "#             for lt, lp in zip(lbl_true, lbl_pred):\n",
    "#                 acc, acc_cls, mean_iu, fwavacc = \\\n",
    "#                     torchfcn.utils.label_accuracy_score(\n",
    "#                         [lt], [lp], n_class=n_class)\n",
    "#                 metrics.append((acc, acc_cls, mean_iu, fwavacc))\n",
    "#             metrics = np.mean(metrics, axis=0)\n",
    "\n",
    "            if iteration >= max_iter:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
